"""Measure quality on several top repositories."""
from argparse import ArgumentParser, Namespace
from collections import OrderedDict
import csv
from datetime import datetime
import functools
import importlib
import io
import json
import logging
import logging.handlers
import os
import subprocess
import sys
import tempfile
from typing import Iterable, Iterator, NamedTuple, Optional, Sequence, Type, Union

from dulwich import porcelain
from lookout.core import slogging
from lookout.core.analyzer import Analyzer
from lookout.core.cmdline import ArgumentDefaultsHelpFormatterNoNone, create_model_repo_from_args
from lookout.core.data_requests import DataService
from lookout.core.event_listener import EventListener
from lookout.core.manager import AnalyzerManager
from lookout.core.test_helpers import server
import numpy
from tabulate import tabulate

from lookout.style.format.benchmarks.general_report import QualityReportAnalyzer
from lookout.style.format.feature_extractor import FeatureExtractor


# TODO: add to ./benchmarks/data/quality_report_repos.csv after bblfsh python client v3 is released and we use it  # noqa: E501
# https://github.com/vuejs/vue,b7105ae8c9093e36ec89a470caa3b78bda3ef467,db1d0474997e9e60b8b0d39a3b7c2af55cfd0d4a",  # noqa: E501
# https://github.com/vuejs/vuex,2e62705d4bce4ebcb8eca23df8c7b849125fc565,1ac16a95c574f6b1386016fb6d4f00cfd2ee1d60",  # noqa: E501

FLOAT_PRECISION = ".3f"


class AnalyzerContextManager:
    """Context manager for launching analyzer."""

    def __init__(
            self, port: int, db: str, fs: str, config: str = "",
            analyzer: Union[str, Sequence[str], Iterable[Type[Analyzer]]] = "lookout.style.format",
            init: bool = True) -> None:
        """
        Init analyzer: model_repository, data_service, arguments, etc.

        :param port: port to use for analyzer.
        :param db: database location.
        :param fs: location where to store results of launched analyzer.
        :param config: Path to the configuration file with option defaults. If empty - skip.
        :param analyzer: analyzer(s) to use.
        :param init: To run `analyzer init` or not. \
                     If you want to reuse existing database set False.
        """
        self.port = port
        self.db = db
        self.fs = fs
        self.config_path = config  # mimic TestAnalyzer - not used so far
        if isinstance(analyzer, (str, type)):
            self.analyzer = [analyzer]
        if isinstance(self.analyzer[0], str):
            self.analyzer = [importlib.import_module(a).analyzer_class for a in self.analyzer]

        class Args:
            pass
        self.args = Namespace()
        self.args.db = "sqlite:///%s" % self.db
        self.args.fs = self.fs
        self.args.cache_size = "1G"
        self.args.cache_ttl = "6h"
        self.args.db_kwargs = {}
        self.args.workers = 1
        # initialize model repository
        self.model_repository = create_model_repo_from_args(self.args)
        if init:
            self.model_repository.init()
        # initialize a new instance of DataService
        data_request_address = "0.0.0.0:10301"
        self.data_service = DataService(data_request_address)

    def __enter__(self) -> "AnalyzerContextManager":
        self.manager = AnalyzerManager(
            analyzers=self.analyzer,
            model_repository=self.model_repository,
            data_service=self.data_service,
        )
        self.listener = EventListener(address="0.0.0.0:%d" % self.port, handlers=self.manager,
                                      n_workers=self.args.workers)
        self.listener.start()
        return self

    def __exit__(self, exc_type=None, exc_val=None, exc_tb=None):
        self.listener.stop()
        self.model_repository.shutdown()
        self.data_service.shutdown()


def get_repo_name(url: str) -> str:
    """
    Extract name of repository from URL.

    :param url: URL for repository.
    :return: name of repository.
    """
    return url.split("/")[-1]


def ensure_repo(repository: str, storage_dir: str) -> str:
    """
    Clones repository if it is an url and returns repository path.

    :param repository: Repository url or directory in the file system.
    :param storage_dir: Clone repository to this directory if it is an url.
    :return: Repository path.
    """
    if os.path.exists(repository):
        return repository
    # clone repository
    git_dir = os.path.join(storage_dir, get_repo_name(repository))  # location for code
    porcelain.clone(repository, git_dir)
    return git_dir


class QualityReport:
    """
    Storage for reports generated by QualityReportAnalyzer.
    """

    def __init__(self, train_report: Optional[str] = None, model_report: Optional[str] = None,
                 test_report: Optional[str] = None) -> None:
        """
        Init method.

        :param train_report: Train report string generated by generate_train_report.
        :param model_report: Model report string generated by generate_model_report.
        :param test_report: Test report string generated by generate_test_report.
        """
        self.train_report = train_report
        self.model_report = model_report
        self.test_report = test_report


class RestartReport(ValueError):
    """Exception raises if report collection should be restarted."""


def measure_quality(repository: str, from_commit: str, to_commit: str, port: int,
                    review_config: dict, train_config: dict, bblfsh: Optional[str],
                    vnodes_expected_number: Optional[int], restarts: int=3) -> QualityReport:
    """
    Generate `QualityReport` for a repository. If it fails it returns empty reports.

    :param repository: URL of repository.
    :param from_commit: Hash of the base commit.
    :param to_commit: Hash of the head commit.
    :param port: Port for QualityReportAnalyzer.
    :param review_config: config for review.
    :param train_config: config for train.
    :param bblfsh: Babelfish server address to use. Specify None to use the default value.
    :param vnodes_expected_number: Specify number for expected number of vnodes if known. \
                                   report collection will be restarted if number of extracted \
                                   vnodes does not match.
    :param restarts: Number of restarts if number of extracted vnodes does not match.
    :return: Reports.
    """
    report = QualityReport()
    log = logging.getLogger("QualityAnalyzer")

    # This dirty hack should be removed as soon as
    # https://github.com/src-d/style-analyzer/issues/557 resolved.
    sum_vnodes_number = 0
    call_numbers = 0

    _convert_files_to_xy_backup = FeatureExtractor._convert_files_to_xy

    def _convert_files_to_xy(self, parsed_files):
        nonlocal sum_vnodes_number, call_numbers
        call_numbers += 1
        sum_vnodes_number += sum(len(vn) for vn, _, _ in parsed_files)
        # sum_vnodes_number + 1 because of whatever reason if you extract test and train
        # separately you have -1 vnode
        # TODO (zurk): investigate ^
        if call_numbers == 2 and sum_vnodes_number + 1 != vnodes_expected_number:
            raise RestartReport("VNodes number does not match to expected: %d != %d:" % (
                sum_vnodes_number, vnodes_expected_number))
        log.info("VNodes number match to expected %d. ", vnodes_expected_number)
        return _convert_files_to_xy_backup(self, parsed_files)

    def capture_report(func, name):
        @functools.wraps(func)
        def wrapped_capture_quality_report(*args, **kwargs):
            if getattr(report, name) is not None:
                raise RuntimeError("%s should be called only one time." % func.__name__)
            result = func(*args, **kwargs)
            setattr(report, name, result)
            return result
        wrapped_capture_quality_report.original = func
        return wrapped_capture_quality_report
    reports = {
        "model_report": "generate_model_report",
        "train_report": "generate_train_report",
        "test_report": "generate_test_report",
    }
    try:
        for name in reports:
            setattr(QualityReportAnalyzer, reports[name],
                    capture_report(getattr(QualityReportAnalyzer, reports[name]), name))
        if vnodes_expected_number:
            log.info("Vnodes expected number is equal to %d", vnodes_expected_number)
            FeatureExtractor._convert_files_to_xy = _convert_files_to_xy
        with tempfile.TemporaryDirectory(prefix="top-repos-quality-repos-") as tmpdirname:
            git_dir = ensure_repo(repository, tmpdirname)
            for attempt_number in range(restarts):
                sum_vnodes_number = -1
                try:
                    server.run(
                       "push", fr=from_commit, to=to_commit, port=port, git_dir=git_dir,
                       log_level="warning", bblfsh=bblfsh, config_json=json.dumps(train_config))
                    break
                except subprocess.CalledProcessError:
                    # Assume that we failed because VNodes number does not match to expected one
                    log.warning("%d/%d try to train the model failed.", attempt_number, restarts)
            else:
                raise RuntimeError("Run out of %d attempts. Failed to train proper model for %s." %
                                   (restarts, repository))
            server.run("review", fr=from_commit, to=to_commit, port=port, git_dir=git_dir,
                       log_level="warning", bblfsh=bblfsh,
                       config_json=json.dumps(review_config))
    finally:
        for name in reports:
            setattr(QualityReportAnalyzer, reports[name],
                    getattr(QualityReportAnalyzer, reports[name]).original)
        if vnodes_expected_number:
            FeatureExtractor._convert_files_to_xy = _convert_files_to_xy_backup
    return report


def calc_weighted_avg(arr: Union[Sequence[Sequence], numpy.ndarray], col: int,
                      weight_col: int = 5) -> float:
    """Calculate average value in `col` weighted by column `weight_col`."""
    arr = numpy.array(arr)
    weights_ = arr[:, weight_col].astype(float)
    col_ = arr[:, col].astype(float)
    numerator = (col_ * weights_).sum()
    denominator = weights_.sum()
    if denominator == 0:
        return 1
    return numerator / denominator


def calc_avg(arr: Union[Sequence[Sequence], numpy.ndarray], col: int) -> float:
    """Calculate average value in `col`."""
    return numpy.array(arr)[:, col].astype(float).sum() / len(arr)


Metrics = NamedTuple("Metrics", (
    ("precision", float),
    ("recall", float),
    ("full_recall", float),
    ("f1", float),
    ("full_f1", float),
    ("ppcr", float),
    ("support", int),
    ("full_support", int),
))
Metrics.__doc__ = """Metrics for the quality report. Metrics are calculated on the samples
 subset where predictions were made. `full_` prefix means that metric was calculated on all
 available samples. Without `full_` means that metric was calculated only on samples where it has
 prediction from the model. `ppcr` means predicted positive condition rate and shows the
 ratio of samples where the model was able to predict.
"""


def _get_metrics(report: str) -> Metrics:
    """Extract avg / total precision, recall, f1 score, support from report."""
    data = _get_json_data(report)
    avg = data["cl_report"]["micro avg"]
    avg_full = data["cl_report_full"]["micro avg"]
    return Metrics(
        precision=avg["precision"], recall=avg["recall"], full_recall=avg_full["recall"],
        f1=avg["f1-score"], full_f1=avg_full["f1-score"], ppcr=data["ppcr"],
        support=avg["support"], full_support=avg_full["support"])


def _get_model_summary(report: str) -> (int, float):
    """Extract model summary - number of rules and avg. len."""
    data = _get_json_data(report)
    # TODO(vmarkovtsev): address this embarrasing hardcode
    return data["javascript"]["num_rules"], data["javascript"]["avg_rule_len"]


def _get_json_data(report: str) -> dict:
    start_anchor = "```json\n"
    mrr_start = report.find(start_anchor, report.rfind("</summary>"))
    if mrr_start < 0:
        raise ValueError("malformed report")
    mrr_start += len(start_anchor)
    mrr_end = report.find("\n```", mrr_start)
    data = json.loads(report[mrr_start:mrr_end])
    return data


def handle_input_arg(input_arg: str, log: Optional[logging.Logger] = None) -> Iterator[str]:
    """
    Process input argument and return an iterator over input data.

    :param input_arg: file to process or `-` to get data from stdin.
    :param log: Logger if you want to log handling process.
    :return: An iterator over input files.
    """
    log = log.info if log else (lambda *x: None)
    if input_arg == "-":
        log("Reading file paths from stdin.")
        for line in sys.stdin:
            yield line
    else:
        with open(input_arg) as f:
            for line in f:
                yield line


def main(args):
    """Entry point for quality report generation."""
    os.makedirs(args.output, exist_ok=True)
    assert os.path.isdir(args.output), "Output should be a directory"
    slogging.setup(args.log_level, False)
    log = logging.getLogger("QualityAnalyzer")
    handler = logging.handlers.RotatingFileHandler(os.path.join(args.output, "errors.txt"))
    handler.setLevel(logging.ERROR)
    log.addHandler(handler)
    if not server.exefile.exists():
        server.fetch()  # download executable
    reports = []
    port = server.find_port()
    review_config = {QualityReportAnalyzer.name: {"aggregate": True}}
    train_config = json.loads(args.train_config)
    repositories = list(csv.DictReader(handle_input_arg(args.input)))
    with tempfile.TemporaryDirectory() as tmpdirname:
        database = args.database if args.database else os.path.join(tmpdirname, "db.sqlite3")
        fs = args.fs if args.fs else os.path.join(tmpdirname, "models")
        os.makedirs(fs, exist_ok=fs)
        with AnalyzerContextManager(port=port, db=database, fs=fs,
                                    analyzer="lookout.style.format.benchmarks.general_report",
                                    init=False):
            start_time = datetime.now()
            for ri, row in enumerate(repositories):
                now = datetime.now()
                if ri > 0:
                    left = (len(repositories) - ri) / ri * (now - start_time)
                else:
                    left = None
                log.info("\n%s\n"
                         "= %-76s =\n"
                         "= %2d / %2d%s=\n"
                         "= Now:  %-60s%s=\n"
                         "= Left: %-40s%s=\n"
                         "= Ends: %-60s%s=\n"
                         "%s",
                         "=" * 80,
                         row["url"],
                         ri + 1, len(repositories), " " * 70,
                         now, " " * 11,
                         left, " " * 31,
                         now + left if left is not None else None, " " * 11,
                         "=" * 80,
                         )
                report_loc = os.path.join(args.output, get_repo_name(row["url"]))
                train_rep_loc = report_loc + ".train_report.md"
                model_rep_loc = report_loc + ".model_report.md"
                test_rep_loc = report_loc + ".test_report.md"
                # generate or read report
                try:
                    if args.force or not os.path.exists(train_rep_loc) or \
                            not os.path.exists(model_rep_loc):
                        # Skip this step if report was already generated
                        vnodes_expected_number = int(row["vnodes_number"]) \
                            if "vnodes_number" in row else None
                        report = measure_quality(
                            row["url"], to_commit=row["to"], from_commit=row["from"], port=port,
                            review_config=review_config, train_config=train_config,
                            bblfsh=args.bblfsh,
                            vnodes_expected_number=vnodes_expected_number)
                        if report.train_report is not None:
                            with open(train_rep_loc, "w", encoding="utf-8") as f:
                                f.write(report.train_report)
                        if report.model_report is not None:
                            with open(model_rep_loc, "w", encoding="utf-8") as f:
                                f.write(report.model_report)
                        if report.test_report is not None:
                            with open(test_rep_loc, "w", encoding="utf-8") as f:
                                f.write(report.test_report)
                    else:
                        log.info("Found existing reports for %s in %s", row["url"], args.output)
                        report = QualityReport()
                        with open(train_rep_loc, encoding="utf-8") as f:
                            report.train_report = f.read()
                        with open(model_rep_loc, encoding="utf-8") as f:
                            report.model_report = f.read()
                        with open(test_rep_loc, encoding="utf-8") as f:
                            report.test_report = f.read()
                    if (report.train_report is not None and
                            report.model_report is not None and
                            report.test_report is not None):
                        reports.append((row["url"], report))
                    else:
                        log.warning("skipped %s: train_report %s, model_report %s, test_report %s",
                                    row["url"], report.train_report is not None,
                                    report.model_report is not None,
                                    report.test_report is not None)
                except Exception:
                    log.exception("-" * 20 + "\nFailed to process %s repo", row["url"])
                    continue

        # precision, recall, f1, support, n_rules, avg_len stats
        additional_fields = ("Rules Number", "Average Rule Len")
        for report_name in ("train_report", "test_report"):
            table = []
            fields2id = OrderedDict()
            with io.StringIO() as output:
                for repo, report in reports:
                    metrics = _get_metrics(getattr(report, report_name))
                    if not table:
                        table.append(("repo",) + metrics._fields + additional_fields)
                        for i, field in enumerate(table[0]):
                            fields2id[field] = i
                    n_rules, avg_len = _get_model_summary(report.model_report)
                    table.append((get_repo_name(repo),) + metrics + (n_rules, avg_len))
                avgvals = tuple(calc_avg(table[1:], fields2id[field]) for field in metrics._fields)
                average = tuple(("%" + FLOAT_PRECISION) % v for v in avgvals[:-2])
                average += tuple("%d" % v for v in avgvals[-2:])  # support, full_support
                average += tuple(("%d", "%.1f")[i] % calc_avg(table[1:], fields2id[field])
                                 for i, field in enumerate(additional_fields))
                fields_to_weight = (
                    ("precision", "support"), ("recall", "support"),
                    ("full_recall", "full_support"), ("f1", "support"),
                    ("full_f1", "full_support"), ("ppcr", "support"),
                )
                weighted_average = []
                for field, weight_field in fields_to_weight:
                    weighted_average.append(("%" + FLOAT_PRECISION) % calc_weighted_avg(
                        table[1:], col=fields2id[field], weight_col=fields2id[weight_field]))
                table.append(("average",) + average)
                table.append(("weighted average",) + tuple(weighted_average))
                float_fields = ("precision", "recall", "full_recall", "f1", "full_f1", "ppcr")
                floatfmts = []
                for field in fields2id:
                    if field in float_fields:
                        floatfmts.append(FLOAT_PRECISION)
                    elif field == "Average Rule Len":
                        floatfmts.append(".1f")
                    else:
                        floatfmts.append("g")

                print(tabulate(table, tablefmt="pipe", headers="firstrow", floatfmt=floatfmts),
                      file=output)
                summary = output.getvalue()
            print(report_name)
            print(summary)
            summary_loc = os.path.join(args.output, "summary-%s.md" % report_name)
            with open(summary_loc, "w", encoding="utf-8") as f:
                f.write(summary)


def create_parser() -> ArgumentParser:
    """Create command line arguments for quality report generation entry point."""
    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatterNoNone)
    parser.add_argument(
        "-i", "--input", required=True,
        help="csv file with repositories to make report. Should contain url, to and from columns.")
    parser.add_argument(
        "-o", "--output", required=True,
        help="Directory where to save results.")
    parser.add_argument(
        "-f", "--force", action="store_true",
        help="If this flag is used - force to overwrite results stored in output directory. "
             "If not - stored results will be used if they exist.")
    parser.add_argument(
        "-b", "--bblfsh", help="Bblfsh address to use.")
    parser.add_argument(
        "--train-config", default="{}",
        help="Config for analyzer train in json format.")
    parser.add_argument(
        "--database", default=None, help="sqlite3 database path to store the models.")
    parser.add_argument(
        "--fs", default=None, help="Model repository file system root.")
    parser.add_argument(
        "--log-level", default="DEBUG", help="Logging level")
    return parser


if __name__ == "__main__":
    parser = create_parser()
    args = parser.parse_args()
    main(args)
